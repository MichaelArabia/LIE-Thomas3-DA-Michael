{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Didn't work, need PyTorch (wich needs CUDA). I installed CUDA, then try to installed PyTorch, but it didn't work. SO, I'm not going to use this for now.\n",
    "# I installed the latest version of CUDA (12.3) and, It seems that PyTorch doesn't support it yet.\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# text = \"Hello, my name is\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\Desktop\\becode\\LIE-Thomas3-DA-Michael\\personal-computer\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mRoger, you asked: 'What's the meaning of life'. Here's my response:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: \n",
      "\n",
      "The meaning of life is that there is no meaning. Each of us gives life the meaning we choose.\n",
      "\n",
      "For me, the meaning of life is to create. To make something that didn't exist before. Whether it is a piece of art, a piece of code, a family or a business.\n",
      "\n",
      "But I am not sure if there's a universal meaning of life. I think each of us should search for our own meaning.\n",
      "\n",
      "And if we don\n"
     ]
    }
   ],
   "source": [
    "# Second version of the chatbot\n",
    "# Define a more dynamic prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user\", \"question\"],\n",
    "    template=\"{user}, you asked: '{question}'. Here's my response:\"\n",
    ")\n",
    "\n",
    "hub_llm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                         model_kwargs={\"temperature\": 0.7, \"max_length\": 250})\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Define a function to handle multi-turn conversations\n",
    "def chatbot_response(user_name, user_input):\n",
    "    inputs = {\n",
    "        \"user\": user_name,\n",
    "        \"question\": user_input\n",
    "    }\n",
    "    response = hub_chain.run(**inputs)\n",
    "    conversation_history.append(f\"User: {user_name}\")\n",
    "    conversation_history.append(f\"Question: {user_input}\")\n",
    "    conversation_history.append(f\"Answer: {response}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "# Create the langchain instance\n",
    "hub_chain = LLMChain(prompt=prompt, llm=hub_llm, verbose=True)\n",
    "\n",
    "user_name = None\n",
    "\n",
    "# Handle multi-turn conversations by saving and loading conversation history\n",
    "while True:\n",
    "    try:\n",
    "        # Take user inputs interactively\n",
    "        if user_name is None:\n",
    "            user_name = input(\"Enter your name: \")\n",
    "        user_question = input(f\"{user_name}, what is your question? \")\n",
    "        if user_question.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Get bot response and print\n",
    "        response = chatbot_response(user_name, user_question)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "        # Save conversation history\n",
    "        with open(\"conversation_history.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(conversation_history))\n",
    "\n",
    "        # Load conversation history without newline characters\n",
    "        with open(\"conversation_history.txt\", \"r\") as file:\n",
    "            conversation_history = file.read().splitlines()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version of the chatbot\n",
    "\n",
    "# hub_llm = HuggingFaceHub(repo_id=\"gpt2-xl\",\n",
    "#                          model_kwargs={\"temperature\":0.7, \"max_length\":250})\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"you asked : {question} . Here's my answer : \"\n",
    "# )\n",
    "\n",
    "# hub_chain = LLMChain(prompt=prompt, llm=hub_llm, verbose=True)\n",
    "# print(hub_chain.run(\"What is the meaning of life?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
